{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentimaster\n",
    "\n",
    "A sentiment analysis project for a competition-based hiring process. \n",
    "\n",
    "Here I investigate the application of a BERT-based approach for the tweets sentiment classification task. The model choice was based on the reports of  \t\n",
    "<http://nlpprogress.com/english/sentiment_analysis.html> and specifically <https://doi.org/10.48550/arXiv.1905.05583>.\n",
    "\n",
    "Nonetheless, a baseline approach using TF-IDF preprocessing with a random forest classifier was also implemented to compare the adequacy of the more sophisticated BERT-based strategy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"QT @user In the original draft of the 7th boo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Ben Smith / Smith (concussion) remains out of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sorry bout the stream last night I crashed out...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chase Headley's RBI double in the 8th inning o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@user Alciato: Bee will invest 150 million in ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  label\n",
       "0  \"QT @user In the original draft of the 7th boo...      2\n",
       "1  \"Ben Smith / Smith (concussion) remains out of...      1\n",
       "2  Sorry bout the stream last night I crashed out...      1\n",
       "3  Chase Headley's RBI double in the 8th inning o...      1\n",
       "4  @user Alciato: Bee will invest 150 million in ...      2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "random_state = 42\n",
    "train_file = \"/home/colombelli/temp/applications/ey/data-set/train_complete.csv\"\n",
    "df = pd.read_csv(train_file)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigation of the basic dataset properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:  47615\n",
      "Labels:\n",
      "1    21542\n",
      "2    18668\n",
      "0     7405\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Tweet NaN values:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of samples: \", len(df))\n",
    "print(\"Labels:\")\n",
    "print(df['label'].value_counts())\n",
    "print(\"\\nTweet NaN values: \", df['tweet'].isna().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing \n",
    "\n",
    "I will use a baseline approach and compare to a state-of-the-art approach for sentiment analysis. \n",
    "The data preprocessing for the TF-IDF approach used with the baseline algorithm is heavier than the preprocessing performed in the data used by BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/colombelli/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer \n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                            reduce_len=True)\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "# There are lots of these functions available on the internet\n",
    "def text_preprocessing_tfidf(text):\n",
    "\n",
    "    # Remove @mentions\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "    # Remove old style retweet text \"RT\"\n",
    "    text = re.sub(r'^RT[\\s]+', '', text)\n",
    "    # Remove hyperlinks\n",
    "    text = re.sub(r'https?://[^\\s\\n\\r]+', '', text)\n",
    "    # Remove the hash # sign from hashtags\n",
    "    text = re.sub(r'#', '', text)\n",
    "\n",
    "    text_clean = []\n",
    "    for word in tokenizer.tokenize(text):\n",
    "        if (word not in stopwords.words('english') and  # Remove stopwords\n",
    "            word not in string.punctuation):  # Remove punctuation\n",
    "\n",
    "            stem_word = stemmer.stem(word) # happy, happiness, etc -> happi            \n",
    "            text_clean.append(stem_word)\n",
    "\n",
    "    return \" \".join(text_clean)\n",
    "\n",
    "\n",
    "# This process can take some time and could be improved\n",
    "def get_tfidf_preprocessed_dataset(df):\n",
    "    df['tweet'] = df['tweet'].map(text_preprocessing_tfidf)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qt origin draft 7th book remu lupin surviv bat...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ben smith smith concuss remain lineup thursday...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sorri bout stream last night crash tonight sur...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chase headley' rbi doubl 8th inning david pric...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alciato bee invest 150 million januari anoth 2...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  label\n",
       "0  qt origin draft 7th book remu lupin surviv bat...      2\n",
       "1  ben smith smith concuss remain lineup thursday...      1\n",
       "2  sorri bout stream last night crash tonight sur...      1\n",
       "3  chase headley' rbi doubl 8th inning david pric...      1\n",
       "4  alciato bee invest 150 million januari anoth 2...      2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df = get_tfidf_preprocessed_dataset(df)\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing_bert(text):\n",
    "\n",
    "    # Remove @mentions\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "    # Replace '&amp;' with '&'\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "    # Remove trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_bert_preprocessed_dataset(df):\n",
    "    df['tweet'] = df['tweet'].map(text_preprocessing_bert)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"QT In the original draft of the 7th book, Rem...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Ben Smith / Smith (concussion) remains out of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sorry bout the stream last night I crashed out...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chase Headley's RBI double in the 8th inning o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alciato: Bee will invest 150 million in Januar...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  label\n",
       "0  \"QT In the original draft of the 7th book, Rem...      2\n",
       "1  \"Ben Smith / Smith (concussion) remains out of...      1\n",
       "2  Sorry bout the stream last night I crashed out...      1\n",
       "3  Chase Headley's RBI double in the 8th inning o...      1\n",
       "4  Alciato: Bee will invest 150 million in Januar...      2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_df = get_bert_preprocessed_dataset(df)\n",
    "bert_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import numpy as np\n",
    "\n",
    "label_encoder = LabelBinarizer()\n",
    "label_encoder.fit(df['label'].values)\n",
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X = tfidf_df['tweet'].values\n",
    "y = tfidf_df['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, \n",
    "                                                    random_state=random_state)\n",
    "\n",
    "tf_idf = TfidfVectorizer(ngram_range=(1, 3),\n",
    "                         binary=True)\n",
    "\n",
    "X_train_tfidf = tf_idf.fit_transform(X_train)\n",
    "X_test_tfidf = tf_idf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance (F1): 0.20529961730368648\n",
      "Accuracy: 0.44498110037799243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/colombelli/miniconda3/envs/sentimaster/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=random_state)\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "f1 = precision_recall_fscore_support(y_test, y_pred, average='macro', beta=1)[2]\n",
    "print(\"Model performance (F1):\", f1)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT state-of-the-art evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization \n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "X = bert_df['tweet'].values\n",
    "y = label_encoder.transform(bert_df['label'].values)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, \n",
    "                                                    random_state=random_state)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=random_state)\n",
    "\n",
    "train_tf_df = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(32)\n",
    "val_tf_df = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(32)\n",
    "test_tf_df = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
     ]
    }
   ],
   "source": [
    "tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1'\n",
    "tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
    "\n",
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)\n",
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "def build_classifier_model():\n",
    "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "  encoder_inputs = preprocessing_layer(text_input)\n",
    "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "  outputs = encoder(encoder_inputs)\n",
    "  net = outputs['pooled_output']\n",
    "  net = tf.keras.layers.Dropout(0.1)(net)\n",
    "  net = tf.keras.layers.Dense(3, activation='softmax', name='classifier')(net)\n",
    "  return tf.keras.Model(text_input, net)\n",
    "\n",
    "\n",
    "classifier_model = build_classifier_model()\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "metrics = [tfa.metrics.F1Score(num_classes=3, average='macro'),\n",
    "            tf.keras.metrics.CategoricalAccuracy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_tf_df).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')\n",
    "\n",
    "\n",
    "classifier_model.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1072/1072 [==============================] - 484s 448ms/step - loss: 1.0118 - f1_score: 0.3763 - categorical_accuracy: 0.4957 - val_loss: 0.8717 - val_f1_score: 0.4636 - val_categorical_accuracy: 0.5811\n",
      "Epoch 2/5\n",
      "1072/1072 [==============================] - 472s 441ms/step - loss: 0.8828 - f1_score: 0.5130 - categorical_accuracy: 0.5836 - val_loss: 0.8097 - val_f1_score: 0.5671 - val_categorical_accuracy: 0.6237\n",
      "Epoch 3/5\n",
      "1072/1072 [==============================] - 440s 410ms/step - loss: 0.8295 - f1_score: 0.5654 - categorical_accuracy: 0.6184 - val_loss: 0.7927 - val_f1_score: 0.5836 - val_categorical_accuracy: 0.6317\n",
      "Epoch 4/5\n",
      "1072/1072 [==============================] - 435s 405ms/step - loss: 0.8023 - f1_score: 0.5868 - categorical_accuracy: 0.6337 - val_loss: 0.7869 - val_f1_score: 0.5972 - val_categorical_accuracy: 0.6398\n",
      "Epoch 5/5\n",
      "1072/1072 [==============================] - 442s 412ms/step - loss: 0.7876 - f1_score: 0.5957 - categorical_accuracy: 0.6421 - val_loss: 0.7830 - val_f1_score: 0.5981 - val_categorical_accuracy: 0.6397\n"
     ]
    }
   ],
   "source": [
    "history = classifier_model.fit(x=train_tf_df,\n",
    "                               validation_data=val_tf_df,\n",
    "                               epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.save(\"models/x_train.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149/149 [==============================] - 18s 122ms/step - loss: 0.7889 - f1_score: 0.6001 - categorical_accuracy: 0.6417\n"
     ]
    }
   ],
   "source": [
    "evaluation = classifier_model.evaluate(test_tf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.7888618111610413\n",
      "F1-Score:  0.6001226305961609\n",
      "Accuracy:  0.6417471766471863\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss: \", evaluation[0])\n",
    "print(\"F1-Score: \", evaluation[1])\n",
    "print(\"Accuracy: \", evaluation[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As expected, the BERT results were far better than the baseline's. Note that this result could be easily improved if I had enough computational resources to train larger BERT models for longer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the challange predictions with the fine-tuned BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "import tensorflow_hub as hub\n",
    "from official.nlp import optimization \n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "\n",
    "def text_preprocessing_bert(text):\n",
    "\n",
    "    # Remove @mentions\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "    # Replace '&amp;' with '&'\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "    # Remove trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "train_file = \"/home/colombelli/temp/applications/ey/data-set/train_complete.csv\"\n",
    "df = pd.read_csv(train_file)\n",
    "label_encoder = LabelBinarizer()\n",
    "label_encoder.fit(df['label'].values)\n",
    "\n",
    "with open('../data-set/test_text.txt') as f: \n",
    "    challenge_test_tweets = np.array([\n",
    "            text_preprocessing_bert(line.rstrip()) for line in f\n",
    "        ])\n",
    "\n",
    "model = tf.keras.models.load_model(\n",
    "            \"models/x_train.h5\", \n",
    "            custom_objects={'KerasLayer':hub.KerasLayer,\n",
    "                            'AdamWeightDecay': optimization.AdamWeightDecay})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfortunatelly, the following command crashes on my PC due to memory limits\n",
    "# An analogous model was trained using all the provided data through Google Colab\n",
    "# The execution details can be found in the colab_runtime.ipynb notebook\n",
    "predictions = model(tf.constant(challenge_test_tweets))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentimaster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 15:19:38) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa90fcad2776796395d7fa3646854c666649353987c4eaf60a11fd1da00b6956"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
